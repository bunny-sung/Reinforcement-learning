{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6o-bgEk9O2"
      },
      "source": [
        "# REINFORCE in PyTorch\n",
        "\n",
        "Just like we did before for Q-learning, this time we'll design a PyTorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
        "\n",
        "Most of the code in this notebook is taken from approximate Q-learning, so you'll find it more or less familiar and even simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "id": "Krup7Q2yk9PE"
      },
      "outputs": [],
      "source": [
        "import sys, os\n",
        "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
        "\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n",
        "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n",
        "\n",
        "    !touch .setup_complete\n",
        "\n",
        "# This code creates a virtual display to draw game images on.\n",
        "# It will have no effect if your machine has a monitor.\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
        "    !bash ../xvfb start\n",
        "    os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "gxTsmsEgk9PJ"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDjc-gjCk9PK"
      },
      "source": [
        "A caveat: with some versions of `pyglet`, the following cell may crash with `NameError: name 'base' is not defined`. The corresponding bug report is [here](https://github.com/pyglet/pyglet/issues/134). If you see this error, try restarting the kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "-xHaqRHnk9PL",
        "outputId": "45e14266-3f75-4034-e973-1cc942a2adc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f4fd194f490>"
            ]
          },
          "metadata": {},
          "execution_count": 270
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "# gym compatibility: unwrap TimeLimit\n",
        "if hasattr(env, '_max_episode_steps'):\n",
        "    env = env.env\n",
        "\n",
        "env.reset()\n",
        "n_actions = env.action_space.n\n",
        "state_dim = env.observation_space.shape\n",
        "\n",
        "plt.imshow(env.render(\"rgb_array\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_me4Juhk9PM"
      },
      "source": [
        "# Building the network for REINFORCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP_Z7vlfk9PM"
      },
      "source": [
        "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states.\n",
        "\n",
        "For numerical stability, please __do not include the softmax layer into your network architecture__.\n",
        "We'll use softmax or log-softmax where appropriate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "-yKx1u5-k9PN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "xTmryI_jk9PT"
      },
      "outputs": [],
      "source": [
        "# Build a simple neural network that predicts policy logits. \n",
        "# Keep it simple: CartPole isn't worth deep architectures.\n",
        "h_dim = 60\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(state_dim[0], h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, h_dim),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h_dim, n_actions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hRsCmAhk9PV"
      },
      "source": [
        "#### Predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmSQ7Mwok9PW"
      },
      "source": [
        "Note: output value of this function is not a torch tensor, it's a numpy array.\n",
        "So, here gradient calculation is not needed.\n",
        "<br>\n",
        "Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
        "to suppress gradient calculation.\n",
        "<br>\n",
        "Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n",
        "<br>\n",
        "With `.detach()` computational graph is built but then disconnected from a particular tensor,\n",
        "so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n",
        "<br>\n",
        "In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "Gya6tIdik9PX"
      },
      "outputs": [],
      "source": [
        "def predict_probs(states):\n",
        "    \"\"\" \n",
        "    Predict action probabilities given states.\n",
        "    :param states: numpy array of shape [batch, state_shape]\n",
        "    :returns: numpy array of shape [batch, n_actions]\n",
        "    \"\"\"\n",
        "    # convert states, compute logits, use softmax to get probability\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    with torch.no_grad():\n",
        "        pred_probab= nn.functional.softmax(model(states),dim=1).numpy()\n",
        "\n",
        "    #print(\"pred_probab\", pred_probab)\n",
        "    return pred_probab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "hIXM4WOlk9PZ"
      },
      "outputs": [],
      "source": [
        "test_states = np.array([env.reset() for _ in range(5)])\n",
        "test_probas = predict_probs(test_states)\n",
        "assert isinstance(test_probas, np.ndarray), \\\n",
        "    \"you must return np array and not %s\" % type(test_probas)\n",
        "assert tuple(test_probas.shape) == (test_states.shape[0], env.action_space.n), \\\n",
        "    \"wrong output shape: %s\" % np.shape(test_probas)\n",
        "assert np.allclose(np.sum(test_probas, axis=1), 1), \"probabilities do not sum to 1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnOgkNCRk9PZ"
      },
      "source": [
        "### Play the game\n",
        "\n",
        "We can now use our newly built agent to play the game."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "zIdVjHPxk9Pa"
      },
      "outputs": [],
      "source": [
        "def generate_session(env, t_max=1000):\n",
        "    \"\"\" \n",
        "    Play a full session with REINFORCE agent.\n",
        "    Returns sequences of states, actions, and rewards.\n",
        "    \"\"\"\n",
        "    # arrays to record session\n",
        "    states, actions, rewards = [], [], []\n",
        "    s = env.reset()\n",
        "\n",
        "    for t in range(t_max):\n",
        "        # action probabilities array aka pi(a|s)\n",
        "        action_probs = predict_probs(np.array([s]))[0]\n",
        "        # Sample action with given probabilities.\n",
        "        a = np.random.choice(range(len(action_probs)), p=action_probs)\n",
        "        new_s, r, done, info = env.step(a)\n",
        "\n",
        "        # record session history to train later\n",
        "        states.append(s)\n",
        "        actions.append(a)\n",
        "        rewards.append(r)\n",
        "\n",
        "        s = new_s\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {
        "id": "VxAup1axk9Pa"
      },
      "outputs": [],
      "source": [
        "# test it\n",
        "states, actions, rewards = generate_session(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KObU09UJk9Pb"
      },
      "source": [
        "### Computing cumulative rewards\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "G_t &= r_t + \\gamma r_{t + 1} + \\gamma^2 r_{t + 2} + \\ldots \\\\\n",
        "&= \\sum_{i = t}^T \\gamma^{i - t} r_i \\\\\n",
        "&= r_t + \\gamma * G_{t + 1}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {
        "id": "IW8u38mek9Pb"
      },
      "outputs": [],
      "source": [
        "def get_cumulative_rewards(rewards,  # rewards at each step\n",
        "                           gamma=0.99  # discount for reward\n",
        "                           ):\n",
        "    \"\"\"\n",
        "    Take a list of immediate rewards r(s,a) for the whole session \n",
        "    and compute cumulative returns (a.k.a. G(s,a) in Sutton '16).\n",
        "    \n",
        "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
        "\n",
        "    A simple way to compute cumulative rewards is to iterate from the last\n",
        "    to the first timestep and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
        "\n",
        "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
        "    \"\"\"\n",
        "    G = np.zeros((len(rewards),))\n",
        "    G_t = 0\n",
        "    for i in range(len(rewards)-1, -1, -1):\n",
        "        G_t = gamma*G_t + rewards[i]\n",
        "        G[i] = G_t\n",
        "    return G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oKtzHSjk9Pc",
        "outputId": "a4d36d6d-f3e1-48db-e8ca-58f06ed8d79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "looks good!\n"
          ]
        }
      ],
      "source": [
        "get_cumulative_rewards(rewards)\n",
        "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
        "    [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
        "    [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
        "assert np.allclose(\n",
        "    get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
        "    [0, 0, 1, 2, 3, 4, 0])\n",
        "print(\"looks good!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZFarM2k9Pc"
      },
      "source": [
        "#### Loss function and updates\n",
        "\n",
        "We now need to define objective and update over policy gradient.\n",
        "\n",
        "Our objective function is\n",
        "\n",
        "$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n",
        "\n",
        "REINFORCE defines a way to compute the gradient of the expected reward with respect to policy parameters. The formula is as follows:\n",
        "\n",
        "$$ \\nabla_\\theta \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\nabla_\\theta \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "We can abuse PyTorch's capabilities for automatic differentiation by defining our objective function as follows:\n",
        "\n",
        "$$ \\hat J(\\theta) \\approx { 1 \\over N } \\sum_{s_i, a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G_t(s_i, a_i) $$\n",
        "\n",
        "When you compute the gradient of that function with respect to network weights $\\theta$, it will become exactly the policy gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "vI3a1HTYk9Pc"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(y_tensor, ndims):\n",
        "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
        "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
        "    y_one_hot = torch.zeros(\n",
        "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
        "    return y_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {
        "id": "Lakg1hTfk9Pd"
      },
      "outputs": [],
      "source": [
        "# Your code: define optimizers\n",
        "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
        "\n",
        "\n",
        "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
        "    \"\"\"\n",
        "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
        "    Updates agent's weights by following the policy gradient above.\n",
        "    Please use Adam optimizer with default parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    # cast everything into torch tensors\n",
        "    states = torch.tensor(states, dtype=torch.float32)\n",
        "    actions = torch.tensor(actions, dtype=torch.int32)\n",
        "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
        "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
        "    # predict logits, probas and log-probas using an agent.\n",
        "    logits = model(states)\n",
        "    probs = nn.functional.softmax(logits, -1)\n",
        "    log_probs = nn.functional.log_softmax(logits, -1)\n",
        "\n",
        "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
        "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
        "\n",
        "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
        "    log_probs_for_actions = torch.sum(\n",
        "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
        "   \n",
        "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
        "    J = torch.mean(log_probs_for_actions * cumulative_returns)\n",
        "\n",
        "    # regularize with entropy\n",
        "    entropy = torch.sum(probs*log_probs)\n",
        "    loss = - (J + entropy_coef * entropy) \n",
        "\n",
        "    # Gradient descent step\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # technical: return session rewards to print them later\n",
        "    return np.sum(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQbqoDZNk9Pe"
      },
      "source": [
        "### The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzV3SNvMk9Pf",
        "outputId": "1a2f43b3-82bb-49c3-cfff-e39507fc669d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean reward:32.700\n",
            "mean reward:46.220\n",
            "mean reward:53.880\n",
            "mean reward:88.320\n",
            "mean reward:157.570\n",
            "mean reward:111.420\n",
            "mean reward:152.990\n",
            "mean reward:134.950\n",
            "mean reward:150.400\n",
            "mean reward:174.190\n",
            "mean reward:142.590\n",
            "mean reward:188.340\n",
            "mean reward:177.610\n",
            "mean reward:414.340\n",
            "You Win!\n"
          ]
        }
      ],
      "source": [
        "for i in range(100):\n",
        "    rewards = [train_on_session(*generate_session(env)) for _ in range(100)]  # generate new sessions\n",
        "    \n",
        "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
        "    \n",
        "    if np.mean(rewards) > 300:\n",
        "        print(\"You Win!\")  # but you can train even further\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQtGmDJtk9Pl"
      },
      "source": [
        "### Results & video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "id": "5e_rvGDWk9Pm"
      },
      "outputs": [],
      "source": [
        "# Record sessions\n",
        "\n",
        "import gym.wrappers\n",
        "\n",
        "with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n",
        "    sessions = [generate_session(env_monitor) for _ in range(100)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/videos/openaigym.video.0.61.video000064.mp4": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "5DFhoAMok9Pm",
        "outputId": "120cb19d-152d-47aa-fc94-d6606d7d017d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<video width=\"640\" height=\"480\" controls>\n",
              "  <source src=\"videos/openaigym.video.0.61.video000064.mp4\" type=\"video/mp4\">\n",
              "</video>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 266
        }
      ],
      "source": [
        "# Show video. This may not work in some setups. If it doesn't\n",
        "# work for you, you can download the videos and view them locally.\n",
        "\n",
        "from pathlib import Path\n",
        "from IPython.display import HTML\n",
        "\n",
        "video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=\"640\" height=\"480\" controls>\n",
        "  <source src=\"{}\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\".format(video_names[-1]))  # You can also try other indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMxCf_f_k9Pm",
        "outputId": "6b32ac78-f7ea-4949-f13d-a8a74d3829b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your average reward is 163.02 over 100 episodes\n",
            "Submission to Coursera returned with HTTP status code 401.\n",
            "You can try generating a new token and make sure you spelled it correctly.\n",
            "Here is the full response:\n",
            "{'details': None,\n",
            " 'errorCode': 'invalidEmailOrToken',\n",
            " 'message': 'Invalid email or token.'}\n"
          ]
        }
      ],
      "source": [
        "from submit import submit_cartpole\n",
        "submit_cartpole(generate_session, 'your.email@example.com', 'YourAssignmentToken')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0NWxGuNk9Pn"
      },
      "source": [
        "That's all, thank you for your attention!\n",
        "\n",
        "Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "practice_reinforce_pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}